{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a936238b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module_path = /hpc/aklicka/Python/modules\n",
      "data_path = /hpc/aklicka/Python-data/training-set-1\n"
     ]
    }
   ],
   "source": [
    "# Define path where to find the module. This allows for a different path depending on where the code is running (my mac or the cluster)\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Define candidate paths\n",
    "module_path_list = [\n",
    "    '/Users/steven/academic-iCloud/Python/modules',\n",
    "    '/hpc/aklicka/Python/modules'\n",
    "]\n",
    "\n",
    "data_path_list = [\n",
    "    '/Users/steven/Python-data',\n",
    "    '/hpc/aklicka/Python-data/training-set-1'\n",
    "]\n",
    "\n",
    "# Resolve actual paths\n",
    "module_path = next((p for p in module_path_list if os.path.exists(p)), None)\n",
    "data_path = next((p for p in data_path_list if os.path.exists(p)), None)\n",
    "\n",
    "# Check and report missing paths\n",
    "if module_path is None:\n",
    "    print(\"Error: Could not locate a valid module path.\")\n",
    "if data_path is None:\n",
    "    print(\"Error: Could not locate a valid data path.\")\n",
    "\n",
    "if module_path is None or data_path is None:\n",
    "    sys.exit(1)\n",
    "\n",
    "# Print resolved paths\n",
    "print(f\"module_path = {module_path}\")\n",
    "print(f\"data_path = {data_path}\")\n",
    "\n",
    "# Reduce TensorFlow verbosity\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "bc6be3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "\n",
      "Python version: 3.9.21 (main, Dec 11 2024, 16:24:11) \n",
      "[GCC 11.2.0]\n",
      "TensorFlow version: 2.4.1\n",
      "TensorFlow is built with CUDA: True\n",
      "TensorFlow is built with ROCm: False\n",
      "\n",
      "System: Linux 4.18.0-553.22.1.el8_10.x86_64 (x86_64)\n",
      "Platform: Linux-4.18.0-553.22.1.el8_10.x86_64-x86_64-with-glibc2.28\n",
      "Processor: x86_64\n",
      "\n",
      "Number of GPUs available to TensorFlow: 1\n",
      "GPU Device: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "\n",
      ">>> Running with GPU available <<<  (Linux-4.18.0-553.22.1.el8_10.x86_64-x86_64-with-glibc2.28)\n",
      "\n",
      "Current time 2025-06-11 11:17:07\n"
     ]
    }
   ],
   "source": [
    "# # Ensure modules are reloaded \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import standard modules\n",
    "import numpy as np\n",
    "\n",
    "import platform\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Add custom module path to list\n",
    "sys.path.append(module_path)\n",
    "\n",
    "# Import custom module\n",
    "import SRSML24.data_prep as dp\n",
    "import SRSML24.model as m\n",
    "import SRSML24.utils as ut\n",
    "\n",
    "import tensorflow as tf\n",
    "#from tensorflow.keras.optimizers.legacy import Adam \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "import skimage as ski\n",
    "import skimage.morphology as morphology\n",
    "import skimage\n",
    "from skimage import morphology, measure\n",
    "\n",
    "#import platform \n",
    "\n",
    "m.print_system_info()\n",
    "\n",
    "start_time = dp.current_datetime()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dad4eea",
   "metadata": {},
   "source": [
    "### Programme variable setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c77bab7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for windows creation\n",
    "# General\n",
    "job_name = 'June_25_wnet'\n",
    "verbose = False             # Set this True to print out more information\n",
    "\n",
    "# MTRX preprocessing\n",
    "flatten_method = 'poly_xy'\n",
    "pixel_density = 15.0        #Â Convert all images to a constant pixel density\n",
    "pixel_ratio = 0.7           # If an image has less than this % in the slow scan direction it is discarded\n",
    "data_scaling = 1.e9         # Scale the z-height of the data\n",
    "\n",
    "# Windowing\n",
    "window_size = 32            # Window size for training/validation\n",
    "window_pitch = 8            # Window pitch for training/validation\n",
    "\n",
    "# Data saving options\n",
    "save_windows = True         # Save the windows as numpy files\n",
    "together = True             # Set this True to save image windows for a mtrx image as a single file rather than separate files.\n",
    "save_jpg = False            # Save the full image as a jpg\n",
    "collate = False             # Set this True to remove all subfolder directories and save all data in root data path\n",
    "save_window_jpgs = False    # Save the windows as jpgs for inspection\n",
    "\n",
    "# Parameters for training\n",
    "model_name = 'unet_' + job_name\n",
    "batch_size = 128\n",
    "buffer_size = 12800 # shuffling\n",
    "learning_rate = 1e-4\n",
    "epochs = 5\n",
    "\n",
    "# Parameters for clustering\n",
    "cluster_model_name = model_name + '_kmeans'\n",
    "cluster_batch_size = 5120 # This is the number of latent features in a batch for clustering. \n",
    "                          # Does not have to be the same as for training and probably should \n",
    "                          # be larger. \n",
    "cluster_buffer_size = cluster_batch_size * 5    # shuffling buffer\n",
    "num_clusters=20                                 # Desired number of clusters (centroids) to form in the data.\n",
    "max_iter=1000                                   # Maximum iterations allowed for each mini-batch to refine centroids.\n",
    "reassignment_ratio=0.05                         # Fraction of clusters reassigned per step; lower values stabilize updates.\n",
    "\n",
    "# Parameters for PREDICTIONS\n",
    "predict_window_pitch = 2                        # Window pitch for prediction\n",
    "predictions_batch_size = 2**15                  # Batch size for predictions\n",
    "\n",
    "\n",
    "# DATA LIMITS FOR TESTING THE CODE\n",
    "mtrx_train_data_limit = 200                    # Number of MTRX files to process (training)\n",
    "mtrx_test_data_limit = 50                     # Number of MTRX files to process (validation)\n",
    "\n",
    "train_data_limit = None                         # Limit the data used in the autoencoder training\n",
    "test_data_limit = None                          # Limit the data used in the autoencoder training (validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "267a13b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_data_path = dp.create_new_data_path(data_path, job_name, include_date=False)\n",
    "\n",
    "mtrx_train_path = os.path.join(data_path, 'mtrx/train')\n",
    "mtrx_test_path = os.path.join(data_path, 'mtrx/test')\n",
    "mtrx_predict_path = os.path.join(data_path, 'mtrx/predict')\n",
    "\n",
    "model_path = os.path.join(job_data_path,'model')\n",
    "cluster_model_path = os.path.join(job_data_path,'cluster_model')\n",
    "\n",
    "latent_features_path = os.path.join(job_data_path, 'latent_features')\n",
    "predict_latent_features_path = os.path.join(job_data_path, 'latent_features_predictions')\n",
    "\n",
    "windows_train_path = os.path.join(job_data_path, 'windows/train')\n",
    "windows_test_path = os.path.join(job_data_path, 'windows/test')\n",
    "windows_predict_path = os.path.join(job_data_path, 'windows/predict')\n",
    "\n",
    "predictions_path = os.path.join(job_data_path, f'predictions')\n",
    "\n",
    "feature_windows_path = os.path.join(job_data_path, 'feature_windows')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896e0ace",
   "metadata": {},
   "source": [
    "### Process Matrix format data to windows for autoencoder training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f691a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp.delete_data_folders(job_data_path, subdirectories=[\"jpg\", \"windows\", \"windows-jpg\"], override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03deb62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "mtrx_train_file_list, _ = dp.list_files_by_extension(mtrx_train_path,'Z_mtrx',verbose=False)\n",
    "\n",
    "dp.process_mtrx_files(\n",
    "    mtrx_train_file_list[0:mtrx_train_data_limit],\n",
    "    job_data_path, # save data path\n",
    "    flatten_method = flatten_method, pixel_density = pixel_density, pixel_ratio = pixel_ratio,\n",
    "    data_scaling = data_scaling, window_size = window_size, window_pitch = window_pitch,\n",
    "    save_windows = save_windows,\n",
    "    save_window_jpgs=save_window_jpgs,\n",
    "    save_jpg = save_jpg,\n",
    "    together = together,\n",
    "    collate = collate,\n",
    "    verbose = verbose\n",
    "    )\n",
    "\n",
    "# Test data\n",
    "mtrx_test_file_list, _ = dp.list_files_by_extension(mtrx_test_path,'Z_mtrx',verbose=False)\n",
    "\n",
    "dp.process_mtrx_files(\n",
    "    mtrx_test_file_list[0:mtrx_test_data_limit],\n",
    "    job_data_path, # save data path\n",
    "    flatten_method = flatten_method, pixel_density = pixel_density, pixel_ratio = pixel_ratio,\n",
    "    data_scaling = data_scaling, window_size = window_size, window_pitch = window_pitch,\n",
    "    save_windows = save_windows,\n",
    "    save_window_jpgs=save_window_jpgs,\n",
    "    save_jpg = save_jpg,\n",
    "    together = together,\n",
    "    collate = collate,\n",
    "    verbose = verbose\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e67aca",
   "metadata": {},
   "source": [
    "### Build tensorflow data pipeline for training and validation of autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ab508f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data - tensorflow data pipeline for autoencoder\n",
    "train_files, num_train = dp.list_files_by_extension(windows_train_path, 'npy')\n",
    "train_files = train_files[:train_data_limit]\n",
    "\n",
    "# Create dataset with prefetching\n",
    "train_dataset = m.create_tf_dataset_batched(\n",
    "    train_files, \n",
    "    batch_size=batch_size, \n",
    "    buffer_size=buffer_size, \n",
    "    window_size=window_size,\n",
    "    is_autoencoder=True, \n",
    "    shuffle=True)\n",
    "\n",
    "# Validation data - tensorflow data pipeline for autoencoder\n",
    "test_files, num_test = dp.list_files_by_extension(windows_test_path, 'npy')\n",
    "test_files = test_files[:test_data_limit]\n",
    "\n",
    "# Create dataset with prefetching\n",
    "test_dataset = m.create_tf_dataset_batched(\n",
    "    test_files, \n",
    "    batch_size=batch_size, \n",
    "    buffer_size=buffer_size, \n",
    "    window_size=window_size,\n",
    "    is_autoencoder=True, \n",
    "    shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
