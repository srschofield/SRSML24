{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN autoencoder and Clustering from MTRX data\n",
    "\n",
    "Use this notebook to load Scienta Omicron Matrix format SPM data and create standardised images for machine learning training and analysis. The code can generate both JPG image data, useful for manually checking the data, and windowed numpy data that can be loaded into ML models. \n",
    "\n",
    "The notebook then creates an autoencoder for training on a large dataset, followed by KMEANS clustering. \n",
    "\n",
    "**Author**: Steven R. Schofield  \n",
    "**Created**: November, 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module_path = /hpc/srs/Python/modules\n",
      "data_path = /hpc/srs/Python-data\n"
     ]
    }
   ],
   "source": [
    "# Define path where to find the module. This allows for a different path depending on where the code is running (my mac or the cluster)\n",
    "import os\n",
    "\n",
    "module_path_list = [\n",
    "    '/Users/steven/academic-iCloud/Python/modules',\n",
    "    '/hpc/srs/Python/modules'\n",
    "] \n",
    "\n",
    "data_path_list = [\n",
    "    '/Users/steven/Python-data',\n",
    "    '/hpc/srs/Python-data'\n",
    "]\n",
    "\n",
    "module_path = next((p for p in module_path_list if os.path.exists(p)), None)\n",
    "if not module_path:\n",
    "    exit(\"No valid module paths.\")\n",
    "else:\n",
    "    print('module_path = {}'.format(module_path))\n",
    "\n",
    "data_path = next((p for p in data_path_list if os.path.exists(p)), None)\n",
    "if not module_path:\n",
    "    exit(\"No valid data paths.\")\n",
    "else:\n",
    "    print('data_path = {}'.format(data_path))\n",
    "\n",
    "# adjust tensorflow output level\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # 0 default all messages, 1 warnings and errors, 2, errors, 3 fatal errors only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Python version: 3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:45:29) \n",
      "[GCC 10.4.0]\n",
      "TensorFlow version: 2.4.1\n",
      "TensorFlow is built with CUDA: True\n",
      "TensorFlow is built with ROCm: False\n",
      "\n",
      "System: Linux 4.18.0-513.24.1.el8_9.x86_64 (x86_64)\n",
      "Platform: Linux-4.18.0-513.24.1.el8_9.x86_64-x86_64-with-glibc2.28\n",
      "Processor: x86_64\n",
      "\n",
      "Number of GPUs available to TensorFlow: 1\n",
      "GPU Device: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "\n",
      ">>> Running with GPU available <<<  (Linux-4.18.0-513.24.1.el8_9.x86_64-x86_64-with-glibc2.28)\n",
      "\n",
      "Current time 2024-12-04 11:07:14\n"
     ]
    }
   ],
   "source": [
    "# # Ensure modules are reloaded \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import standard modules\n",
    "import os, sys\n",
    "import numpy as np\n",
    "\n",
    "import platform\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Add custom module path to list\n",
    "sys.path.append(module_path)\n",
    "\n",
    "# Import custom module\n",
    "import SRSML24.data_prep as dp\n",
    "import SRSML24.model as m\n",
    "\n",
    "import tensorflow as tf\n",
    "#from tensorflow.keras.optimizers.legacy import Adam \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import platform \n",
    "\n",
    "m.print_system_info()\n",
    "\n",
    "start_time = dp.current_datetime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for windows creation\n",
    "job_name = 'all_data_2023'\n",
    "job_data_path = dp.create_new_data_path(data_path, job_name, include_date=False)\n",
    "mtrx_train_path = os.path.join(data_path, 'mtrx/train')\n",
    "mtrx_test_path = os.path.join(data_path, 'mtrx/test')\n",
    "mtrx_predict_path = os.path.join(data_path, 'mtrx/predict')\n",
    "flatten_method = 'poly_xy'\n",
    "pixel_density = 15.0    #Â Convert all images to a constant pixel density\n",
    "pixel_ratio = 0.7       # If an image has less than this % in the slow scan direction it is discarded\n",
    "data_scaling = 1.e9     # Scale the z-height of the data\n",
    "window_size = 32        # Window size for training/validation\n",
    "window_pitch = 32       # Window pitch for training/validation\n",
    "together = True        # Set this True to save image windows for a mtrx image as a single file rather than separate files.\n",
    "collate = False         # Set this True to remove all subfolder directories and save all data in root data path\n",
    "\n",
    "# Parameters for training\n",
    "model_name = 'unet_' + job_name\n",
    "batch_size = 128\n",
    "buffer_size = 12800 # shuffling\n",
    "learning_rate = 1e-4\n",
    "epochs = 5\n",
    "\n",
    "# Parameters for clustering\n",
    "cluster_model_name = model_name + '_kmeans'\n",
    "cluster_batch_size = 5120 # This is the number of latent features in a batch for clustering. \n",
    "                          # Does not have to be the same as for training and probably should \n",
    "                          # be larger. \n",
    "cluster_buffer_size = cluster_batch_size * 5    # shuffling buffer\n",
    "num_clusters=20                                # Desired number of clusters (centroids) to form in the data.\n",
    "n_init=50                                       # Number of times the algorithm will run with different centroid seeds.\n",
    "max_iter=1000                                   # Maximum iterations allowed for each mini-batch to refine centroids.\n",
    "reassignment_ratio=0.05                         # Fraction of clusters reassigned per step; lower values stabilize updates.\n",
    "\n",
    "\n",
    "# Parameters for PREDICTIONS\n",
    "predict_window_pitch = 4               # Window pitch for prediction\n",
    "\n",
    "\n",
    "# DATA LIMITS FOR TESTING THE CODE\n",
    "mtrx_train_data_limit = None #500         # Number of MTRX files to process (training)\n",
    "mtrx_test_data_limit = None #100         # Number of MTRX files to process (validation)\n",
    "\n",
    "train_data_limit = None #2000*batch_size\n",
    "predict_data_limit = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted: /hpc/srs/Python-data/all_data_2023/jpg\n",
      "Deleted: /hpc/srs/Python-data/all_data_2023/windows\n",
      "All specified folders have been successfully deleted.\n"
     ]
    }
   ],
   "source": [
    "# REMOVE ALL DATA FOLDERS EXCEPT MTRX \n",
    "dp.delete_data_folders(job_data_path, override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 62592 files with extension 'Z_mtrx' in directory:\n",
      "/hpc/srs/Python-data/mtrx/train\n",
      "There are 62592 files to process\n",
      "....t...t....t....................t.t............ttt.......t.......t.....ttt.t............tt..tttt. 100\n",
      "t........t......ttt..t..tt.ttt.........t..t....tt.t.tt..tt...tttttttttttttttttttttt.ttt.ttt.tt..ttt 200\n",
      "\btt..t..tttt...tt.tttt.t.t.tttttttt.t.tt.tt.t..t.tt..t.......t.t....ttt...........ttt........t...t... 300\n",
      "t....t.............t...............tt..................t........t......ttt.ttt...t....t..t.t....t.t 400\n",
      "........tttttttttttttt...tttttttttttt........t........ttttttt....tttt.tt.......ttttt.t............. 500\n",
      ".t.t.t.t..................t....tttttttttt.tt....t.t.t..ttt.t.t...t.t.t.t.t..t..t.t.t..t..t.t.t.t... 600\n",
      "tt..t..t.tt.t..t.t.t.t.t.t..t..t....tt.t.t..t......................t..t.t.......................... 700\n",
      ".....tt.t..ttttt.........t.....tt.....t..t..t.tt.t....tt...tt..t.t.tttt...tt..t.tt.t..t..t........t 800\n",
      ".....tt.tttt..ttttt.t...t..tt.....t..t..t...t..t..t.tt..t.t..t...t..t.....t...t..t..t.t..t..tttt.t. 900\n",
      "\bttt..t.t....t..t.t...tttt.t.t.t.t.t.tt..t..t..t.tt.ttt..t.t.t...t....ttttttt..ttt.t..t.t.t.t.t..t.t. 1000\n",
      "\bt.t...tttttttt..t...t.tt..t.t.t..t.t.t.t.t..t...tt..t.t.t..ttt..tttt.tt..ttttt..tt.t.tttt.tt.t.tt.t. 1100\n",
      "tttt.t..t..t...t.t.t..t..tt..tttt.t.tt.t.tttt.ttt..............tt.t.tt.tt..t.t....t..t.t..t......tt 1200\n",
      "\bttttt......tt.........t.............t..........tt...t....ttt...tt.t.t.tt.tt..t.tt.tttt..tt..t.t..ttt 1300\n",
      "\btttt.t.t..t.....t.....ttttttttttttttttttttttttttt.t..t.t..ttttt.t.t.ttttttt...t..ttttttt....tt..t..t 1400\n",
      "\btttt......ttttttt.ttt.tt.t.t..tttttt...tt..t..........t................tt..........ttttt.ttttttttttt 1500\n",
      "\bttttttttttttttt.t.ttttt.ttt.t..tttttt.ttttttttt.t..t.t.t...tt.......t.t..tt.ttttt.t....ttt...t.tt..t 1600\n",
      "t.t..ttt..t...tt...t....tt....t..t.t..ttttt.tttt....t......ttttttttttttttttttttttt.ttttttt...tttttt 1700\n",
      "\bttttttttttttttt...t....t..ttttt....tt....ttt.......ttt.....tttt......tttttttttttttt.ttttttt.t.tt.... 1800\n",
      "t.....t........t...t.t...t...t.tt........t.tttttttttttt.ttttttt...t.tttt..tttt..t.t................ 1900\n",
      ".................t.....tttt...t.ttt..t.ttt.ttt..ttttt....t..t.t....tt....t......................... 2000\n",
      ".....t..ttt.t.tt.....t.tttttttt....tttt.tt..t..t.tt.t....t...t.t.......t.t...t..................... 2100\n",
      "..................ttt..........tt...tttttttt....ttt..........t...t......t....t...t..t..t..t...t...t 2200\n",
      ".tt..........t.........t.....tt..t....................t.........ttttt......................tt..t... 2300\n",
      ".t......ttttt.tttttttt.ttt.tt......t..ttt.......................t..........t....tttt.t..t..tt...... 2400\n",
      "............tttttt...tt.ttttttttttttttt...t..............t.....t...tt.tttttt...ttttt.t......t..t... 2500\n",
      "t.t......t.tt.........t...t...tt.tt.t.t..ttt....tt..tt..ttt..tt.ttt..................t............. 2600\n",
      ".....t......................................................t...t..t.t...t..t.....t....t..t.t..t... 2700\n",
      "......................t.tt.t..t..t..ttt....t................tt....ttt..tttt.........t.............. 2800\n",
      "..t....ttt.ttt.ttt....tt..t..t..tt.t...t...t.......t.t..................t..t.t..............ttt.... 2900\n",
      "...........t.....ttt..ttttttt.t........ttt........tt..t.....t...tt..........................ttt.... 3000\n",
      "............t...t.....t.tt.tt........tt..........................................t...t............t 3100\n",
      "...t.t..........................t.....................t............................................ 3200\n",
      "t..........t........t.....tttt.t...t.t...t.t....t..tt.t.t....t...t...t...ttt.tt.....ttt..t.t.t..t.. 3300\n",
      "\bt.t....t.t........t........tt..................t.tt...tt...........t......t......tt..t...t..t....t.. 3400\n",
      "......t.t..t...t.t..t..................t...t..t.................................................... 3500\n",
      "...........tt............................................t....t.....t....t........ttt....tt...t..tt 3600\n",
      ".ttt.........................t.................t................................t.................. 3700\n",
      ".t...tt...........................................................................t............t.tt 3800\n",
      ".............................t..........................t.t......................tt............t... 3900\n",
      "....t..........t.......t.....................t.t.....t.........................t...............t... 4000\n",
      "......................................t...............ttt..t.....t............t.t..tt..t.tt........ 4100\n",
      "................t.....tt.t....t.........t.t......ttt.tt.t.t.tt...tt..t.t......t.tt...t.tttttt..tt.t 4200\n",
      "\bt..t.tt.t.t.t.t.....ttt..t..t......t..t.t.......tt.t.tt.......tt......t.....t.................t..... 4300\n",
      "......t.t..tt........................t..tt.t.ttt....t...tttttt.....t..t..t............ttt...t....t. 4400\n",
      ".t..t..tttttt..t.t.....t...........t..t.ttttt.ttt..t...t..t...t.ttttt....t.t....t...........t.tt... 4500\n",
      ".....tt........t.........t..........t.t.tt..............tttttt.....ttt.tttttttttt..tt.t..ttttttt... 4600\n",
      "..tt..ttt..tttttt.t..tttt..t.........t...................t..............t..tt...tt........t........ 4700\n",
      "......t..ttttt..ttt.t.........t.t..t.......ttt..t.....t.tt...............ttt....................tt. 4800\n",
      "\btttt..........................................tt.t...t.t.t..............t........ttt.ttt..t.tt.t.ttt 4900\n",
      "\btt.ttttt.ttt.t.t.ttt.tttttt...ttttttttttt..t.ttttttttt....t.t..ttt..tttttt...t.tttttt.ttttttttttt.tt 5000\n",
      "\btt.ttt.tttt.ttttt.t.t.....tt.tt....ttt.tt.ttttttt.........t.tt..tttttt.ttt.t.tttt.t.......t.t..tt..t 5100\n",
      "\bttt.tttttt.ttttt.tttttttt.tt..ttttttttttttttttttt....ttttttt.t.........t...tttt..tttttttttttttttt.tt 5200\n",
      "\btttttttt.t...tt..ttt.ttt.t...t.....t.t..t.tttttttt...t............t........t...tttt...............t. 5300\n",
      "..ttttt..t.tt.......t..ttttt.tt..t..........t.........tt.t....ttttttt..............t.ttt.tt.t...... 5400\n",
      "...t....tt.tt.....tt.......ttt..t..t..t..........t.t............t.................................. 5500\n",
      "..............t..tt.t.t..................................................................t.t.t..... 5600\n",
      "...t.t.....................t..t.tt.............t.t.t......t.t....tt.t..tt.tt.t.t.t.t..t.t...tt.t.t. 5700\n",
      "\bt..tt.t.t.t.t.t.t..ttttt.tt.t.t.t.t.t..t.....t.t...tt.t.tt.t.t.t.t.tt.t.tt.t.t.t...t.t.t.ttt.t.tt..t 5800\n",
      ".t.ttt.t.t.t.t.t.t.t.t.t.t..............t.t.tt.t.t.t......t..t................t..........t......... 5900\n",
      "...................................t................t.............tt...t.........t......t.......... 6000\n",
      "\btt....tt..........t.....t..t..t.............tt..t..tt.t......................ttt...........tt.t..... 6100\n",
      "................................................t..ttt.tttt.ttt..t...t...tttttttttttttt.tttttttt.t. 6200\n",
      ".....ttt.t.......t.......t...........................t............................................t 6300\n",
      "t...................ttttttt......t.t.t.t.t.t.t.t..t..tt.t.t.t.tttttttt............................. 6400\n",
      "..........ttttttt.tttttttt..........tj.t........................................................... 6500\n",
      "..tt..t....................t...t.t..t.t.t......t................................................... 6600\n",
      "..........t.................................t...................................tttt.tttttttt...... 6700\n",
      ".tt.t.............tt........t..............t....tjttt.....t..t....tt.....t.......ttt......t........ 6800\n",
      "..................tt....tttt....t.....tt...........t......tt.t.t.t.tt...t..t..tt...t.....tt.t.....t 6900\n",
      "..........t..........................................t.t.tt.t.tttttttt.......t.....tt...t...t...... 7000\n",
      ".......t...t...t..t.........................t...t.......................t...t..t..t.......t........ 7100\n",
      ".......t.t.ttttt.t..tt...t.t.t.t.ttt..tt....t.t...........t...........t...t........tt..t..t.t....t. 7200\n",
      "....t......t..tt....t.t.ttt...........t......t...tt......tttt.t...tttt.ttttttttt.t.ttttttttt.t.tt.t 7300\n",
      "\bt.t.....ttt..tttt....ttt.ttttt........................................tt..............ttt........... 7400\n",
      "..........................................t.t..t........t..........tt...t.t.tt..t.................t 7500\n",
      "............tt.....................................tt.............................................. 7600\n",
      "................................................................................................... 7700\n",
      "..............t.tt.ttt...t....tt.t.t.t.t.t..t...t..t.....................t......................... 7800\n",
      "............................................t...................................................... 7900\n",
      ".........................t..t.............t....t..tt.ttt..ttt.....t.t.......t...ttt.t.ttttt.....t.t 8000\n",
      "t.tt.t.t.t.t.t...t..t.t....t....t.t.t.t.t.t...t...t........t.t.t.t......tt.t.t.t..t.tt...t.t.t.t... 8100\n",
      "\bt...t....tt...........t.....t.ttt.t............t..t..t..........t.tttttttttt.ttt...t...t............ 8200\n",
      "...........................t....................................................................... 8300\n",
      "......................t........................................................t.tttttttttt.tttt... 8400\n",
      "..........tt........................t.tttttt..........t.............t...........t.............t..t. 8500\n",
      ".................t.......t..................t.t.t.....ttttttt.t....t.tt..ttttt..t.................. 8600\n",
      "....t.t...t....t..................................................t...................t.t..t....t.t 8700\n",
      "........t.t.t.tt..t...t.t.......t...........t.tt.tt.....t.tt.ttt.t...t.tt.tttttt.........tttt...... 8800\n",
      ".........................................t...............................................t......... 8900\n",
      "....................tt............tt.......tt.t......................................ttt.......t... 9000\n",
      "....................ttt.t............t..tttttt........t..................t...........t.t......tt.t. 9100\n",
      "...t...........ttt...........tttttttttt.ttttt.ttttttt.....t.t...t.......t..t.t....ttt.tt....t...... 9200\n",
      "ttt.t...t..tt....................t....................t.........t..ttttt.tttt.tt..t.t.t...t........ 9300\n",
      ".............t..t.t..tttt..ttttt...tt..t.t.t.t.t.t.....ttt.t.ttt.tt..........ttt........t.t..t..t.. 9400\n",
      "............t.........tt.tttt.................tt.tt.t..ttttt.t..t.....t.tttt.t.tt.................t 9500\n",
      "........t.t...................t........t.t........t..t.tt.tt..........tt....t................t..t.. 9600\n",
      "....t..........t...............ttttt......t.............................................tt......... 9700\n",
      "......tt........ttttt..t......tt...........t....t.....tt.......ttt................................. 9800\n",
      "t...........................................................t..........t.............t....t..tttttt 9900\n",
      "\btttt....t..................t...............tttttt....................ttttttttt.tttttttt....t..t..tt. 10000\n",
      "t.t.t.......ttt.ttt.t........t..t...tttttttt...tt....................tt....tt.t.ttt................ 10100\n",
      ".t.....ttttttttttttt.ttttttt..t..ttt........t...tt.ttttt..t...............t.t..tt............t..... 10200\n",
      "......t...............................t.....ttt..................................................t. 10300\n",
      "\bt..t.....t.....t.....t..t...ttt..ttttt..tttttttt.tttt.ttttt...ttt..t.....t..t..tttt..t..t........... 10400\n",
      "..............t.t.t...t.t...t.t..t..t.........t......ttttttttt.ttt.tt.t.tttttttttttttttttttttt.t..t 10500\n",
      "\bttt.tttt....tttt.ttttttttttttttttt.ttttttt.t.....t.t..ttttttttttttttt.......................t....... 10600\n",
      ".............t.ttt.tttttt.tttttttt..t.t..t.ttttt.t.t...t..t.....ttttt.tttttttt.tttt..t..t.ttttttttt 10700\n",
      "\btt..tt..t..t....tttttttttttt..tttt...tt.t.tt.t.t.t.t.tt.tttttttt.t..tttttttt...t...ttt.ttt...ttt..t. 10800\n",
      "..t....tt..tttt.t.t..t..t....t...t..t.....t..t....t.t...t..t.....t....t...t..t..t...t..t..t...t..t. 10900\n",
      "\btttttttt..t............tt.t.tt.t.....t..............................tttt.tttttttt.......t...tt....tt 11000\n",
      "\bt......t.t...tttttttt......t...tttt.tttttttt.....t..t.ttt.ttt...tt...t...tt...t..t.t...tt....ttt.... 11100\n",
      "\bt..t.t..........t............tt....tt.t...t..t.............t...tt.t.........t.......t............... 11200\n",
      ".................t..t......................ttt..........tt......t.tt...........t.t....t...tt..tttt. 11300\n",
      ".t.tt..t...t......t...t..t..t..t...t..t...t...t..t.tttttttt..t.t...tt...tttt..t....t..t......t..tt. 11400\n",
      "\bt..t.....................tt.........................t..t..tt...tt....tt..ttt.........t.............. 11500\n",
      ".tt..tt..........tt.t.t...tt.......t.ttt....t...tt..t.................t..........tt..tt.t.......... 11600\n",
      "..............t..t.tttt...ttttttt.ttt..tt.........................................................t 11700\n",
      "\bt.........t...t............................tt....................................................... 11800\n",
      "..........................................t.t.t.........................t.t...................tt... 11900\n",
      "............t..................t...tt.tt....................tt...ttttt..tttt.ttttt...tttt..t....ttt 12000\n",
      "ttttttttttt.............t.tt..tttttt.tt...tt.t.t..t..t...t.t.............t..t...ttt.t..t...tt...t.t 12100\n",
      "........tt..........................t...t.t..............tt....t....ttttt.t....t...........t..t...t 12200\n",
      ".tttt....................t.tt..........................tt...................t........t....tt....... 12300\n",
      "tt..t...t...t.........t.......t.ttt.................t....t..................t...tt.....t......t.... 12400\n",
      "....ttttttt..t....t......t..........t..t...t..ttttttt.tttt.t.....ttttt....tttt...tt......tttt...... 12500\n",
      ".............t.tt.t.tt....tttttttt...ttt....t.t.t.t..t..t...t..............t........t..t.ttt..tttt. 12600\n",
      ".ttttttt..t.ttttt..............................................t...tt................t........t..t. 12700\n",
      "\bt.tttttttttttt.ttt.t....tt.....ttttttt........t..t..ttttt..ttt.....tt....ttt....t...............t... 12800\n",
      "\bt.t........................................t.t......tt......t.......tttt....tt....t.t............... 12900\n",
      ".......................tttttt...t.t......t...t....t..tttt.tt....t..tt......t.........t...t..t...ttt 13000\n",
      "...ttt..tt..t....t........................t..........t..t.........tttttt.tt....t.t....t..ttt..tt... 13100\n",
      "..tt..t.tttt...tt.........ttt..ttt......t.....t..t.t...tt..t........................t.......t...... 13200\n",
      "....t.tt.t..tttt....tt......t...................................................................... 13300\n",
      ".......t........................................................................................... 13400\n",
      ".t......tt............t.......................................................t.....tttttt....t..tt 13500\n",
      "...t.t.....t...t.t........t...t.t.....t.t...t................................t...t....t..t......t.. 13600\n",
      "................tt.......t.........t.....t..ttt..t.tt....tt...t.................................... 13700\n",
      "...................t.tttt..t.t.....tttt.....t..t..........t.......t..t..t....t..t...t..ttt......... 13800\n",
      "..........t............t.t.......tt.t..t....ttttt...tt...tt....t...t..t...t........t.....tttt...... 13900\n",
      "...t..t...tt.t.ttt..t....t................t.....tt...t...........................t................. 14000\n",
      "..........t....tt......t...........t..............tt.tt..t....ttttt..t..t..ttttt..t.....t...t..t... 14100\n",
      "\bt.t.ttttttttt.tt.t...t.t.t...ttttt..t......t.....t..........t...tt...ttt......t.t.tttt.t.t.t.t..t... 14200\n",
      "tt...ttttttttttt.ttttt..tt...tt...ttttttt.t...t.t..t..t..t..t..t...ttt..t.tttttttttt.t..tt..t.t.t.t 14300\n",
      "...t..t..t...t.......t.tttt......t.tttt..t..t..t..t..........t..t...............t...t..tt.......... 14400\n",
      "....t.................t......t..tt.tttt.t......t......................................t.tt.t...t..t 14500\n",
      "\btt...ttt.........ttt.tttt.........t......t........t..t..t.............t..t....tt..ttt.tt.tt..t..t... 14600\n",
      "...................t......t.t..t.t..t..tttt...t..tt................................................ 14700\n",
      "....................t......................................t.....................t.t.......tt...... 14800\n",
      ".......................t.............t...........................................................t. 14900\n",
      "t....t...t.............................................t........................................... 15000\n",
      "...........................t..t............................t..t.t...t........t.t.t..t.............t 15100\n",
      "...t.t..t.t.....t..t.t...t.......................................................................t. 15200\n",
      "......................................t............................................................ 15300\n",
      "...........................................t.................................................t..... 15400\n",
      ".....tt..t..t..ttt..................t..............................................t............... 15500\n",
      "\bttt......t........t.t...................t................................tt...t.t.t.t.....t.......t. 15600\n",
      "...........t.....................................................................t.....tt.......... 15700\n",
      "...................................t.......................t.t.t.........t..t.....................t 15800\n",
      ".t.tt......tt.t..........................................................t......................... 15900\n",
      "................................................................................................... 16000\n",
      "...............................................................t..t.t.t..tttt.t.t.t.t..t.t......... 16100\n",
      "................................................................................................... 16200\n",
      "................................................................................................... 16300\n",
      "....ttt.....................................................................t.....t................ 16400\n",
      "........t............t.................................................tt.......................... 16500\n",
      ".....................................................t..tttt..t.................................... 16600\n",
      "...t...................t..t......................................tttttttttt....................tt.t 16700\n",
      "t.........tt.t.....t............................................................................... 16800\n",
      ".....................................................................................t.......t.ttt. 16900\n",
      "..t.........t.........t..t.tt....t.tttttttt..........tt.ttt.......t...t..t..tt.t...t...ttt.t.ttttt. 17000\n",
      "t.....tt...........t.t...t..t.....t.....t.tttttttt...............ttttttt...t.ttt.tttttttttttt..t..t 17100\n",
      "\bttt.tttt...tt.t.t..t.ttttt.....tt.t......tt.ttttt..t.tt.....ttt..........t.................ttt.t.tt. 17200\n",
      "tt.tt.........t.ttt..t.ttttt..tt....t.t.........................................................t.. 17300\n",
      "..........................t.....t...........................t.t....................t........tttt..t 17400\n",
      "\bttt..t..t....tttt.t.tt.ttt.t..t.tt..tt..tt.t.t.t.tttttttttttt..t.ttttttt..t.tt...tt.t....t.....t..tt 17500\n",
      "t.......tt.t..t.....t.tt....t.ttt.....t.t..t...t.t...ttttt.tt...t.ttt...ttttt.........t.t....t..t.. 17600\n",
      "\bttt.t.t.....................t.........t........tttt.....t.......t.ttt..ttttttttt.tttt....tt...t..t.. 17700\n",
      "\bt..ttt.t......t.tt..........t.......t..t...tttttttt.tt.tttttt.t..ttt.t....tttttt.................... 17800\n",
      ".................t................tt.......t.............t.t..ttt.................................. 17900\n",
      ".................tt...ttt..............................t....t.t.t..t.........t..................... 18000\n",
      "...........t.............................tt..t...t.t.tttt..ttttttt.ttt...........t.t............... 18100\n",
      "t.............t...t...tt.tttt.tttttt.tt.tttttt....................ttt..ttt.tt.......t.............. 18200\n",
      "....t..........tt..........t..t.............................t.............t..t..................... 18300\n",
      "....t.....................t..........tt..........t..ttttttttttttttttttttttttttttttttttttttttttttttt 18400\n",
      "\btttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttt..t.t. 18500\n",
      "\btt.t.t.t.ttttttttttt.tttttt.....t..........t..........t..........t..........ttt....ttt.............. 18600\n",
      "..................ttttt...t......ttttt........................................t..t.tt......ttttt... 18700\n",
      "\bttt......t.t..tt.tttttttttt.ttttttttt..t...........................................tt.........tttttt 18800\n",
      "..tttttt....tt.........................tttt......tttt.............................................. 18900\n",
      ".....t..tttt..................t..tttttt....tttttttttttttttttttttttttttttttttttttttttt.ttttttttttttt 19000\n",
      "\bttt....t...........t.tttttttttttttttt..t..t.ttttt............tt..t......tt..tt..t.t.t.....ttt..t.... 19100\n",
      "\bt.t..........t....t....t....tt..t.t.tttttttt..t...........................t.ttt...tt.ttt.t.tttt.tttt 19200\n",
      "\btttttt...t.t.ttt.t.tt.tt..t.tt.tt.t.ttt.t.t...tttttttt.tttttttt.........t.ttttt.t.tt....t........... 19300\n",
      ".....tt.tttttttt....ttt.....ttt...ttttt..ttttt....ttttt..tttt..t.t....t....t....t.t.t.t....t..t.... 19400\n",
      "tt..t.....t...t.tttttttt..tttt..t...t.t.t.tt..t.t.t..t..t..tt..t.tt..t...t..t..tt.....t....t....... 19500\n",
      ".t.........t.tt.t.tttt.t..t..........tt.tt......t.ttt.......tt.ttt.....t......t.t...t.............. 19600\n",
      "....t.........t...ttt...............t................t....ttttttt................t...ttttttt.tt...t 19700\n",
      "........t..tttttt..ttttt...t.t..t...ttt.t....t.....t.t.t...........t..t.tt...t....t.t......tt.ttt.. 19800\n",
      "ttt.tt..ttt..tt..............t...t.......t......t.ttt.......tt...t......t..tt......t............... 19900\n",
      "..t........t....t....................................................t..t....t.......t..t.....t.t.t 20000\n",
      ".................................t..ttttttt...ttttt..t.....tt....t.t.t...t....ttt..ttttttttttt.t.tt 20100\n",
      "\btttttt..t..t..t..tt..t..t..........t...t......t...t....t.t..j..t..t..ttj..ttt.t..j............t..... 20200\n",
      ".ttt.t..............j..tt..tt..j..................t..t.t.t...t..............tt.t...t.ttt..t........ 20300\n",
      ".t............j...t..t.............t..t.....tt....t....tt..ttt.........t..t...t.tt.tt.t.t....t..... 20400\n",
      "............t...........t.ttttt....tt...........t......ttttttt..............t...........t..t....... 20500\n",
      "..................................t....................t....t..................t................... 20600\n",
      ".ttttttttttt.................t..ttt........t............................................ttttttttttt 20700\n",
      "\btttttttt.ttttttt............ttttttttttttttttttt.ttttttt.................ttttttttttttttttttttttttttt. 20800\n",
      "........t...t......t.....................ttttttttttttttt...ttttttt...................t............. 20900\n",
      ".....................t.......t.t....t...tt..t...t..."
     ]
    }
   ],
   "source": [
    "# Training data\n",
    "mtrx_train_file_list, _ = dp.list_files_by_extension(mtrx_train_path,'Z_mtrx',verbose=False)\n",
    "\n",
    "dp.process_mtrx_files(\n",
    "    mtrx_train_file_list[0:mtrx_train_data_limit],\n",
    "    job_data_path, # save data path\n",
    "    flatten_method = flatten_method, pixel_density = pixel_density, pixel_ratio = pixel_ratio,\n",
    "    data_scaling = data_scaling, window_size = window_size, window_pitch = window_pitch,\n",
    "    save_windows = True,\n",
    "    save_jpg = True,\n",
    "    together = together,\n",
    "    collate = collate,\n",
    "    verbose = False\n",
    "    )\n",
    "\n",
    "# Test data\n",
    "mtrx_test_file_list, _ = dp.list_files_by_extension(mtrx_test_path,'Z_mtrx',verbose=False)\n",
    "\n",
    "dp.process_mtrx_files(\n",
    "    mtrx_test_file_list[0:mtrx_test_data_limit],\n",
    "    job_data_path, # save data path\n",
    "    flatten_method = flatten_method, pixel_density = pixel_density, pixel_ratio = pixel_ratio,\n",
    "    data_scaling = data_scaling, window_size = window_size, window_pitch = window_pitch,\n",
    "    save_windows = True,\n",
    "    save_jpg = True,\n",
    "    together = together,\n",
    "    collate = collate,\n",
    "    verbose = False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up tensorflow data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data - tensorflow data pipeline for autoencoder\n",
    "windows_train_path = os.path.join(job_data_path, 'windows/train')\n",
    "train_files, num_train = dp.list_files_by_extension(windows_train_path, 'npy')\n",
    "train_files = train_files[:train_data_limit]\n",
    "# Create dataset with prefetching\n",
    "#train_dataset = m.create_tf_dataset(train_files, batch_size=batch_size, buffer_size=buffer_size, is_autoencoder=True, shuffle=True)\n",
    "train_dataset = m.create_tf_dataset_batched(\n",
    "    train_files, \n",
    "    batch_size=batch_size, \n",
    "    buffer_size=buffer_size, \n",
    "    window_size=window_size,\n",
    "    is_autoencoder=True, \n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation data - tensorflow data pipeline for autoencoder\n",
    "windows_test_path = os.path.join(job_data_path, 'windows/test')\n",
    "test_files, num_test = dp.list_files_by_extension(windows_test_path, 'npy')\n",
    "test_files = test_files[:train_data_limit]\n",
    "\n",
    "# Create dataset with prefetching\n",
    "test_dataset = m.create_tf_dataset_batched(\n",
    "    test_files, \n",
    "    batch_size=batch_size, \n",
    "    buffer_size=buffer_size, \n",
    "    window_size=window_size,\n",
    "    is_autoencoder=True, \n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder build and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and compile the UNET model\n",
    "autoencoder_model = m.build_autoencoder(window_size=window_size,model_name=model_name)\n",
    "autoencoder_model.summary()\n",
    "\n",
    "# Check if running on Apple Silicon\n",
    "is_mac_silicon = platform.system() == \"Darwin\" and platform.processor() == \"arm\"\n",
    "\n",
    "if is_mac_silicon:\n",
    "    print(\"Running on Mac with Apple Silicon. Using legacy RMSprop optimizer.\")\n",
    "    autoencoder_model.compile(\n",
    "        optimizer=tf.keras.optimizers.legacy.RMSprop(learning_rate=learning_rate),\n",
    "        loss='mean_squared_error',\n",
    "        metrics=['mse', 'mae']\n",
    "    )\n",
    "else:\n",
    "    print(\"Not running on Mac with Apple Silicon. Using new RMSprop optimizer.\")\n",
    "    autoencoder_model.compile(\n",
    "        optimizer=tf.keras.optimizers.RMSprop(learning_rate=learning_rate),\n",
    "        loss='mean_squared_error',\n",
    "        metrics=['mse', 'mae']\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using the tf.data datasets\n",
    "history = autoencoder_model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=test_dataset,\n",
    "    epochs=epochs,\n",
    "    shuffle=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save the model as soon as training completes\n",
    "model_path = os.path.join(job_data_path,'model')\n",
    "m.save_model(autoencoder_model, model_path, model_name=model_name)\n",
    "\n",
    "end_time = dp.current_datetime()\n",
    "dp.elapsed_time(start_time,end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "m.plot_training_history(history, \n",
    "                        loss_name='loss', \n",
    "                        val_loss_name='val_loss', \n",
    "                        metric_names=['mse', 'mae'], \n",
    "                        save_to_disk=True,\n",
    "                        val_metric_names=['val_mse', 'val_mae'],\n",
    "                        output_path=model_path,\n",
    "                        dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract latent features using autoencoder model\n",
    "Will save the latent features also to disk so this step needs to run only once for a given set of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp.delete_data_folders(\n",
    "    job_data_path, \n",
    "    subdirectories='latent_features',\n",
    "    override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data - tensorflow data pipeline \n",
    "windows_train_path = os.path.join(job_data_path, 'windows/train')\n",
    "train_files, num_train = dp.list_files_by_extension(windows_train_path, 'npy')\n",
    "train_files = train_files[:train_data_limit]\n",
    "\n",
    "train_dataset = m.create_tf_dataset_batched(\n",
    "    train_files, \n",
    "    batch_size=cluster_batch_size, \n",
    "    buffer_size=cluster_buffer_size, \n",
    "    window_size=window_size,\n",
    "    is_autoencoder=True, \n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained autoencoder model\n",
    "model_path = os.path.join(job_data_path,'model')\n",
    "autoencoder_model = m.load_model(model_path, model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to save latent features to disk\n",
    "latent_features_path = os.path.join(job_data_path, 'latent_features')\n",
    "\n",
    "m.extract_latent_features_to_disk_from_prebatched_windows(\n",
    "    autoencoder_model, \n",
    "    train_dataset, \n",
    "    latent_features_path, \n",
    "    features_name='latent_features_train',\n",
    "    return_array=False,\n",
    "    verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create tensorflow dataset pipeline for latent features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List and sort latent feature files\n",
    "latent_features_path = os.path.join(job_data_path, 'latent_features')\n",
    "latent_features_files, num_latent_files = dp.list_files_by_extension(latent_features_path, 'npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the latent features from disk into a tensor dataset pipeline\n",
    "latent_features_dataset = m.create_latent_features_tf_dataset(\n",
    "    latent_features_files,\n",
    "    batch_size=cluster_batch_size,\n",
    "    shuffle=True, \n",
    "    shuffle_buffer_size=cluster_buffer_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train KMEANS cluster model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_model, convergence_history = m.train_kmeans(\n",
    "    latent_features_dataset,                # tf.data.Dataset containing batches of latent feature vectors.\n",
    "    batch_size=cluster_batch_size,          # Size of each batch for the KMeans model (controls memory usage and stability).\n",
    "    num_clusters=num_clusters,              # Desired number of clusters (centroids) to form in the data.\n",
    "    n_init=n_init,                          # Number of times the algorithm will run with different centroid seeds.\n",
    "    max_iter=max_iter,                      # Maximum iterations allowed for each mini-batch to refine centroids.\n",
    "    reassignment_ratio=reassignment_ratio   # Fraction of clusters reassigned per step; lower values stabilize updates.\n",
    ")\n",
    "\n",
    "# Save cluster model\n",
    "cluster_model_path = os.path.join(job_data_path,'cluster_model')\n",
    "m.save_cluster_model(cluster_model, cluster_model_path, model_name=cluster_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Predictions MTRX data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp.delete_data_folders(\n",
    "    job_data_path, \n",
    "    subdirectories='jpg',\n",
    "    override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction data\n",
    "mtrx_predict_file_list, _ = dp.list_files_by_extension(mtrx_predict_path,'Z_mtrx',verbose=False)\n",
    "\n",
    "data_scaling = 1.e9     # Scale the z-height of the data\n",
    "\n",
    "flatten_method = 'row_and_poly_xy_deg_2'\n",
    "flatten_method = 'row_mean'\n",
    "flatten_method = 'iterate_mask'\n",
    "flatten_method = 'poly_xy'\n",
    "\n",
    "dp.process_mtrx_files(\n",
    "    mtrx_predict_file_list,\n",
    "    job_data_path, # save data path\n",
    "    flatten_method = flatten_method, pixel_density = pixel_density, pixel_ratio = pixel_ratio,\n",
    "    data_scaling = data_scaling, window_size = window_size, window_pitch = predict_window_pitch,\n",
    "    save_windows = True,\n",
    "    save_jpg = True,\n",
    "    collate = False,\n",
    "    verbose = False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make data predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained autoencoder\n",
    "model_path = os.path.join(job_data_path,'model')\n",
    "autoencoder_model = m.load_model(model_path, model_name=model_name)\n",
    "\n",
    "#Â Load a previously saved cluster model from disk\n",
    "cluster_model_path = os.path.join(job_data_path,'cluster_model')\n",
    "cluster_model = m.load_cluster_model(cluster_model_path, model_name=cluster_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of folders containing predict window data\n",
    "windows_predict_path = os.path.join(job_data_path, 'windows/predict')\n",
    "predict_data_files_list = dp. find_bottom_layer_folders(windows_predict_path)\n",
    "predict_data_files_list = predict_data_files_list[:predict_data_limit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_time_stamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "for dir in predict_data_files_list:\n",
    "    #Limit viewing to FU images\n",
    "    if not dir.endswith(\"FU\"):\n",
    "        print('Not FU so skipping: {}'.format(dir))\n",
    "        continue  \n",
    "\n",
    "    #Â get list of the individual file names of the windows\n",
    "    window_list, num_windows = dp.list_files_by_extension(dir, extension=\".npy\", verbose=False)\n",
    "    # load the numpy windows\n",
    "    image_windows = dp.load_n_numpy_files(window_list,num_windows)\n",
    "    # load the coordinate files for the windows for window reconstruction\n",
    "    image_windows_coordinates = dp.load_coordinates_file(dir)\n",
    "    # Build the reconstruction of the original image\n",
    "    reconstructed_img = dp.reconstruct_image(image_windows,image_windows_coordinates,window_size)\n",
    "\n",
    "    #Â create a tf dataset from the numpy windows for making the latent features\n",
    "    predict_dataset = m.create_tf_dataset(window_list, batch_size=num_windows, buffer_size=num_windows, shuffle=False, is_autoencoder=False)\n",
    "    # make the latent features for each window using the autoencoder model \n",
    "    latent_predict_features, num_latent_predictions = m.extract_latent_features_to_disk_from_prebatched_windows(\n",
    "        autoencoder_model, \n",
    "        predict_dataset, \n",
    "        latent_features_path, \n",
    "        features_name='latent_features_train',\n",
    "        return_array=True,\n",
    "        verbose=False)\n",
    "    # make preductions \n",
    "    cluster_predictions = cluster_model.predict(latent_predict_features)\n",
    "    #Â Build the reconstruction of the predicted cluster label data\n",
    "    cluster_img = dp.reconstruct_cluster_image(image_windows_coordinates,window_size, cluster_predictions)\n",
    "    image_name = os.path.basename(dir)\n",
    "    # Path to save latent features to disk\n",
    "    predictions_path = os.path.join(job_data_path, 'predictions')\n",
    "    m.display_reconstructed_and_cluster_images(reconstructed_img,cluster_img,show_overlay=True,\n",
    "                                                save_to_disk=True,\n",
    "                                                predictions_time_stamp=predictions_time_stamp,\n",
    "                                                output_path=job_data_path,\n",
    "                                                image_name=image_name,\n",
    "                                                dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
