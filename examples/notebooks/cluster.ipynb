{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # CNN autoencoder and Clustering from MTRX data\n",
    "\n",
    "Use this notebook to load Scienta Omicron Matrix format SPM data and create standardised images for machine learning training and analysis. The code can generate both JPG image data, useful for manually checking the data, and windowed numpy data that can be loaded into ML models. \n",
    "\n",
    "The notebook then creates an autoencoder for training on a large dataset, followed by KMEANS clustering. \n",
    "\n",
    "**Author**: Steven R. Schofield  \n",
    "**Created**: November, 2024 -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module_path = /Users/steven/academic-iCloud/Python/modules\n",
      "data_path = /Users/steven/Python-data\n"
     ]
    }
   ],
   "source": [
    "# Define path where to find the module. This allows for a different path depending on where the code is running (my mac or the cluster)\n",
    "import os\n",
    "\n",
    "module_path_list = [\n",
    "    '/Users/steven/academic-iCloud/Python/modules',\n",
    "    '/hpc/srs/Python/modules'\n",
    "] \n",
    "\n",
    "data_path_list = [\n",
    "    '/Users/steven/Python-data',\n",
    "    '/hpc/srs/Python-data'\n",
    "]\n",
    "\n",
    "module_path = next((p for p in module_path_list if os.path.exists(p)), None)\n",
    "if not module_path:\n",
    "    exit(\"No valid module paths.\")\n",
    "else:\n",
    "    print('module_path = {}'.format(module_path))\n",
    "\n",
    "data_path = next((p for p in data_path_list if os.path.exists(p)), None)\n",
    "if not module_path:\n",
    "    exit(\"No valid data paths.\")\n",
    "else:\n",
    "    print('data_path = {}'.format(data_path))\n",
    "\n",
    "# adjust tensorflow output level\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # 0 default all messages, 1 warnings and errors, 2, errors, 3 fatal errors only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Python version: 3.8.18 (default, Sep 11 2023, 08:17:16) \n",
      "[Clang 14.0.6 ]\n",
      "TensorFlow version: 2.13.0\n",
      "TensorFlow is built with CUDA: False\n",
      "TensorFlow is built with ROCm: False\n",
      "\n",
      "System: Darwin 24.1.0 (arm64)\n",
      "Platform: macOS-15.1.1-arm64-arm-64bit\n",
      "Processor: arm\n",
      "\n",
      "Number of GPUs available to TensorFlow: 1\n",
      "GPU Device: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "\n",
      ">>> Running with GPU available <<<  (macOS-15.1.1-arm64-arm-64bit)\n",
      "\n",
      "Current time 2024-12-24 11:02:34\n"
     ]
    }
   ],
   "source": [
    "# # Ensure modules are reloaded \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import standard modules\n",
    "import os, sys\n",
    "import numpy as np\n",
    "\n",
    "import platform\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Add custom module path to list\n",
    "sys.path.append(module_path)\n",
    "\n",
    "# Import custom module\n",
    "import SRSML24.data_prep as dp\n",
    "import SRSML24.model as m\n",
    "\n",
    "import tensorflow as tf\n",
    "#from tensorflow.keras.optimizers.legacy import Adam \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import platform \n",
    "\n",
    "m.print_system_info()\n",
    "\n",
    "start_time = dp.current_datetime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for windows creation\n",
    "job_name = 'MSc2024_Data'\n",
    "job_data_path = dp.create_new_data_path(data_path, job_name, include_date=False)\n",
    "mtrx_train_path = os.path.join(data_path, 'mtrx/train')\n",
    "mtrx_test_path = os.path.join(data_path, 'mtrx/test')\n",
    "mtrx_predict_path = os.path.join(data_path, 'mtrx/predict')\n",
    "flatten_method = 'poly_xy'\n",
    "pixel_density = 15.0    # Convert all images to a constant pixel density\n",
    "pixel_ratio = 0.7       # If an image has less than this % in the slow scan direction it is discarded\n",
    "data_scaling = 1.e9     # Scale the z-height of the data\n",
    "window_size = 32        # Window size for training/validation\n",
    "window_pitch = 32       # Window pitch for training/validation\n",
    "together = True        # Set this True to save image windows for a mtrx image as a single file rather than separate files.\n",
    "collate = False         # Set this True to remove all subfolder directories and save all data in root data path\n",
    "\n",
    "# Parameters for training\n",
    "model_name = 'unet_' + job_name\n",
    "batch_size = 128\n",
    "buffer_size = 12800 # shuffling\n",
    "learning_rate = 1e-4\n",
    "epochs = 5\n",
    "\n",
    "# Parameters for clustering\n",
    "cluster_model_name = model_name + '_kmeans'\n",
    "cluster_batch_size = 5120 # This is the number of latent features in a batch for clustering. \n",
    "                          # Does not have to be the same as for training and probably should \n",
    "                          # be larger. \n",
    "cluster_buffer_size = cluster_batch_size * 5    # shuffling buffer\n",
    "num_clusters=20                                # Desired number of clusters (centroids) to form in the data.\n",
    "n_init=50                                       # Number of times the algorithm will run with different centroid seeds.\n",
    "max_iter=1000                                   # Maximum iterations allowed for each mini-batch to refine centroids.\n",
    "reassignment_ratio=0.05                         # Fraction of clusters reassigned per step; lower values stabilize updates.\n",
    "\n",
    "# Parameters for PREDICTIONS\n",
    "predict_window_pitch = 4               # Window pitch for prediction\n",
    "\n",
    "# DATA LIMITS FOR TESTING THE CODE\n",
    "mtrx_train_data_limit = None #10000        # Number of MTRX files to process (training)\n",
    "mtrx_test_data_limit = None #1000        # Number of MTRX files to process (validation)\n",
    "\n",
    "train_data_limit = None #100                     # Limit the data used in the autoencoder training\n",
    "test_data_limit = None #20                     # Limit the data used in the autoencoder training (validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE ALL DATA FOLDERS EXCEPT MTRX \n",
    "dp.delete_data_folders(job_data_path, subdirectories=[\"jpg\", \"windows\", \"windows-jpg\"], override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "mtrx_train_file_list, _ = dp.list_files_by_extension(mtrx_train_path,'Z_mtrx',verbose=False)\n",
    "\n",
    "dp.process_mtrx_files(\n",
    "    mtrx_train_file_list[0:mtrx_train_data_limit],\n",
    "    job_data_path, # save data path\n",
    "    flatten_method = flatten_method, pixel_density = pixel_density, pixel_ratio = pixel_ratio,\n",
    "    data_scaling = data_scaling, window_size = window_size, window_pitch = window_pitch,\n",
    "    save_windows = True,\n",
    "    save_window_jpgs=True,\n",
    "    save_jpg = True,\n",
    "    together = together,\n",
    "    collate = collate,\n",
    "    verbose = False\n",
    "    )\n",
    "\n",
    "# Test data\n",
    "mtrx_test_file_list, _ = dp.list_files_by_extension(mtrx_test_path,'Z_mtrx',verbose=False)\n",
    "\n",
    "dp.process_mtrx_files(\n",
    "    mtrx_test_file_list[0:mtrx_test_data_limit],\n",
    "    job_data_path, # save data path\n",
    "    flatten_method = flatten_method, pixel_density = pixel_density, pixel_ratio = pixel_ratio,\n",
    "    data_scaling = data_scaling, window_size = window_size, window_pitch = window_pitch,\n",
    "    save_windows = True,\n",
    "    save_window_jpgs=True,\n",
    "    save_jpg = True,\n",
    "    together = together,\n",
    "    collate = collate,\n",
    "    verbose = False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # Set up tensorflow data pipeline -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data - tensorflow data pipeline for autoencoder\n",
    "windows_train_path = os.path.join(job_data_path, 'windows/train')\n",
    "train_files, num_train = dp.list_files_by_extension(windows_train_path, 'npy')\n",
    "train_files = train_files[:train_data_limit]\n",
    "\n",
    "# Create dataset with prefetching\n",
    "train_dataset = m.create_tf_dataset_batched(\n",
    "    train_files, \n",
    "    batch_size=batch_size, \n",
    "    buffer_size=buffer_size, \n",
    "    window_size=window_size,\n",
    "    is_autoencoder=True, \n",
    "    shuffle=True)\n",
    "\n",
    "# Validation data - tensorflow data pipeline for autoencoder\n",
    "windows_test_path = os.path.join(job_data_path, 'windows/test')\n",
    "test_files, num_test = dp.list_files_by_extension(windows_test_path, 'npy')\n",
    "test_files = test_files[:test_data_limit]\n",
    "\n",
    "# Create dataset with prefetching\n",
    "test_dataset = m.create_tf_dataset_batched(\n",
    "    test_files, \n",
    "    batch_size=batch_size, \n",
    "    buffer_size=buffer_size, \n",
    "    window_size=window_size,\n",
    "    is_autoencoder=True, \n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # Autoencoder build and train -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and compile the UNET model\n",
    "autoencoder_model = m.build_autoencoder(window_size=window_size,model_name=model_name)\n",
    "autoencoder_model.summary()\n",
    "\n",
    "# Check if running on Apple Silicon\n",
    "is_mac_silicon = platform.system() == \"Darwin\" and platform.processor() == \"arm\"\n",
    "\n",
    "if is_mac_silicon:\n",
    "    print(\"Detected Mac with Apple Silicon. Compiling the model with the legacy RMSprop optimizer for compatibility with TensorFlow-metal.\")\n",
    "    autoencoder_model.compile(\n",
    "        optimizer=tf.keras.optimizers.legacy.RMSprop(learning_rate=learning_rate),\n",
    "        loss='mean_squared_error',\n",
    "        metrics=['mse', 'mae']\n",
    "    )\n",
    "else:\n",
    "    print(\"Compiling the model with the RMSprop optimizer.\")\n",
    "    autoencoder_model.compile(\n",
    "        optimizer=tf.keras.optimizers.RMSprop(learning_rate=learning_rate),\n",
    "        loss='mean_squared_error',\n",
    "        metrics=['mse', 'mae']\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using the tf.data datasets\n",
    "history = autoencoder_model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=test_dataset,\n",
    "    epochs=epochs,\n",
    "    shuffle=True,\n",
    "    verbose=1\n",
    ")\n",
    "model_train_time = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# Save the model as soon as training completes\n",
    "model_path = os.path.join(job_data_path,'model')\n",
    "m.save_model(autoencoder_model, model_path, model_name=model_name, model_train_time=model_train_time)\n",
    "\n",
    "end_time = dp.current_datetime()\n",
    "dp.elapsed_time(start_time,end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "m.plot_training_history(history, \n",
    "                        loss_name='loss', \n",
    "                        val_loss_name='val_loss', \n",
    "                        metric_names=['mse', 'mae'], \n",
    "                        save_to_disk=True,\n",
    "                        model_name=model_name, \n",
    "                        model_train_time=model_train_time,\n",
    "                        val_metric_names=['val_mse', 'val_mae'],\n",
    "                        output_path=model_path,\n",
    "                        dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # Extract latent features using autoencoder model\n",
    "Will save the latent features also to disk so this step needs to run only once for a given set of parameters -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp.delete_data_folders(\n",
    "    job_data_path, \n",
    "    subdirectories='latent_features',\n",
    "    override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data - tensorflow data pipeline \n",
    "windows_train_path = os.path.join(job_data_path, 'windows/train')\n",
    "train_files, num_train = dp.list_files_by_extension(windows_train_path, 'npy')\n",
    "train_files = train_files[:train_data_limit]\n",
    "\n",
    "train_dataset = m.create_tf_dataset_batched(\n",
    "    train_files, \n",
    "    batch_size=cluster_batch_size, \n",
    "    buffer_size=cluster_buffer_size, \n",
    "    window_size=window_size,\n",
    "    is_autoencoder=True, \n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained autoencoder model\n",
    "model_path = os.path.join(job_data_path,'model')\n",
    "autoencoder_model = m.load_model(model_path, model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to save latent features to disk\n",
    "latent_features_path = os.path.join(job_data_path, 'latent_features')\n",
    "\n",
    "m.extract_latent_features_to_disk_from_prebatched_windows(\n",
    "    autoencoder_model, \n",
    "    train_dataset, \n",
    "    latent_features_path, \n",
    "    features_name='latent_features_train',\n",
    "    return_array=False,\n",
    "    verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # Create tensorflow dataset pipeline for latent features -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 62 files with extension 'npy' in directory:\n",
      "/Users/steven/Python-data/MSc2024_Data/latent_features\n"
     ]
    }
   ],
   "source": [
    "# List and sort latent feature files\n",
    "latent_features_path = os.path.join(job_data_path, 'latent_features')\n",
    "latent_features_files, num_latent_files = dp.list_files_by_extension(latent_features_path, 'npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data pipeline created with 62 files, batch size: 5120\n",
      "Shuffling enabled with buffer size: 25600\n",
      "Batch shape: (5120, 4096)\n"
     ]
    }
   ],
   "source": [
    "# Load the latent features from disk into a tensor dataset pipeline\n",
    "latent_features_dataset = m.create_latent_features_tf_dataset(\n",
    "    latent_features_files,\n",
    "    batch_size=cluster_batch_size,\n",
    "    shuffle=True, \n",
    "    shuffle_buffer_size=cluster_buffer_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # Train KMEANS cluster model -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_model, convergence_history = m.train_kmeans(\n",
    "    latent_features_dataset,                # tf.data.Dataset containing batches of latent feature vectors.\n",
    "    batch_size=cluster_batch_size,          # Size of each batch for the KMeans model (controls memory usage and stability).\n",
    "    num_clusters=num_clusters,              # Desired number of clusters (centroids) to form in the data.\n",
    "    n_init=n_init,                          # Number of times the algorithm will run with different centroid seeds.\n",
    "    max_iter=max_iter,                      # Maximum iterations allowed for each mini-batch to refine centroids.\n",
    "    reassignment_ratio=reassignment_ratio   # Fraction of clusters reassigned per step; lower values stabilize updates.\n",
    ")\n",
    "\n",
    "# Save cluster model\n",
    "cluster_model_path = os.path.join(job_data_path,'cluster_model')\n",
    "m.save_cluster_model(cluster_model, cluster_model_path, model_name=cluster_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # Process Predictions MTRX data -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # Make data predictions -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from: /Users/steven/Python-data/MSc2024_Data/model/unet_MSc2024_Data.keras\n",
      "Cluster model loaded from: /Users/steven/Python-data/MSc2024_Data/cluster_model/unet_MSc2024_Data_kmeans.pkl\n"
     ]
    }
   ],
   "source": [
    "# Load the trained autoencoder\n",
    "model_path = os.path.join(job_data_path,'model')\n",
    "autoencoder_model = m.load_model(model_path, model_name=model_name)\n",
    "\n",
    "# Load a previously saved cluster model from disk\n",
    "cluster_model_path = os.path.join(job_data_path,'cluster_model')\n",
    "cluster_model = m.load_cluster_model(cluster_model_path, model_name=cluster_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No target folders found to delete.\n"
     ]
    }
   ],
   "source": [
    "dp.delete_data_folders(\n",
    "    job_data_path, \n",
    "    subdirectories='windows/predict',\n",
    "    override=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction data\n",
    "mtrx_predict_file_list, _ = dp.list_files_by_extension(mtrx_predict_path,'Z_mtrx',verbose=False)\n",
    "\n",
    "# flatten_method = 'row_and_poly_xy_deg_2'\n",
    "# flatten_method = 'row_mean'\n",
    "# flatten_method = 'iterate_mask'\n",
    "# flatten_method = 'poly_xy'\n",
    "\n",
    "dp.process_mtrx_files(\n",
    "    mtrx_predict_file_list,\n",
    "    job_data_path, # save data path\n",
    "    flatten_method = flatten_method, pixel_density = pixel_density, pixel_ratio = pixel_ratio,\n",
    "    data_scaling = data_scaling, window_size = window_size, \n",
    "    window_pitch = window_pitch,\n",
    "    save_windows = True,\n",
    "    save_window_jpgs=True,\n",
    "    save_jpg = True,\n",
    "    together = together,\n",
    "    collate = collate,\n",
    "    verbose = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training data\n",
    "# mtrx_train_file_list, _ = dp.list_files_by_extension(mtrx_train_path,'Z_mtrx',verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of folders containing predict window data\n",
    "windows_predict_path = os.path.join(job_data_path, 'windows/predict')\n",
    "predict_data_files_list, predict_data_files_num = dp.list_files_by_extension(windows_predict_path,'.npy',verbose=False)\n",
    "image_windows_coordinates_file_list , _ = dp.list_files_by_extension(windows_predict_path,'.txt',verbose=False)\n",
    "# Remove all file names that contain \"coordinates\"\n",
    "image_windows_coordinates_file_list = [\n",
    "    name for name in image_windows_coordinates_file_list \n",
    "    if \"coordinates\" in name\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction data\n",
    "mtrx_predict_file_list, _ = dp.list_files_by_extension(mtrx_predict_path,'Z_mtrx',verbose=False)\n",
    "\n",
    "# flatten_method = 'row_and_poly_xy_deg_2'\n",
    "# flatten_method = 'row_mean'\n",
    "# flatten_method = 'iterate_mask'\n",
    "# flatten_method = 'poly_xy'\n",
    "\n",
    "dp.process_mtrx_files(\n",
    "    mtrx_predict_file_list,\n",
    "    job_data_path, # save data path\n",
    "    flatten_method = flatten_method, pixel_density = pixel_density, pixel_ratio = pixel_ratio,\n",
    "    data_scaling = data_scaling, window_size = window_size, \n",
    "    window_pitch = predict_window_pitch,\n",
    "    save_windows = True,\n",
    "    save_window_jpgs=True,\n",
    "    save_jpg = True,\n",
    "    together = False,\n",
    "    collate = collate,\n",
    "    verbose = False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 47524 files with extension '.npy' in directory:\n",
      "/Users/steven/Python-data/MSc2024_Data/windows/predict/default_2011Feb11-105510_STM-STM_Spectroscopy--110_16_BD\n",
      "Not shuffling\n",
      "Data pipeline created with 47524 files, batch size: 47524\n",
      "Sample batch shape: (47524, 32, 32, 1)\n",
      ".Combined latent features shape: (47524, 4096)\n",
      "Directory created: /Users/steven/Python-data/MSc2024_Data/predictions/2024-12-24_11-16-59\n",
      "Image saved to /Users/steven/Python-data/MSc2024_Data/predictions/2024-12-24_11-16-59/default_2011Feb11-105510_STM-STM_Spectroscopy--110_16_BD.jpg with dpi=150\n",
      "Found 47524 files with extension '.npy' in directory:\n",
      "/Users/steven/Python-data/MSc2024_Data/windows/predict/default_2011Feb11-105510_STM-STM_Spectroscopy--110_16_BU\n",
      "Not shuffling\n",
      "Data pipeline created with 47524 files, batch size: 47524\n",
      "Sample batch shape: (47524, 32, 32, 1)\n",
      ".Combined latent features shape: (47524, 4096)\n",
      "Image saved to /Users/steven/Python-data/MSc2024_Data/predictions/2024-12-24_11-16-59/default_2011Feb11-105510_STM-STM_Spectroscopy--110_16_BU.jpg with dpi=150\n",
      "Found 47524 files with extension '.npy' in directory:\n",
      "/Users/steven/Python-data/MSc2024_Data/windows/predict/default_2011Feb11-105510_STM-STM_Spectroscopy--110_16_FD\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Get list of folders containing predict window data\n",
    "windows_predict_path = os.path.join(job_data_path, 'windows/predict')\n",
    "predict_data_files_list = dp. find_bottom_layer_folders(windows_predict_path)\n",
    "\n",
    "predictions_time_stamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "for dir in predict_data_files_list:\n",
    "    #Limit viewing to FU images\n",
    "    # if not dir.endswith(\"FU\"):\n",
    "    #     print('Not FU so skipping: {}'.format(dir))\n",
    "    #     continue  \n",
    "\n",
    "    # get list of the individual file names of the windows\n",
    "    window_list, num_windows = dp.list_files_by_extension(dir, extension=\".npy\", verbose=False)\n",
    "    # load the numpy windows\n",
    "    image_windows = dp.load_n_numpy_files(window_list,num_windows)\n",
    "    # load the coordinate files for the windows for window reconstruction\n",
    "    image_windows_coordinates = dp.load_coordinates_file(dir)\n",
    "    # Build the reconstruction of the original image\n",
    "    reconstructed_img = dp.reconstruct_image(image_windows,image_windows_coordinates,window_size)\n",
    "\n",
    "    # create a tf dataset from the numpy windows for making the latent features\n",
    "    predict_dataset = m.create_tf_dataset(window_list, batch_size=num_windows, buffer_size=num_windows, shuffle=False, is_autoencoder=False)\n",
    "    # make the latent features for each window using the autoencoder model \n",
    "    latent_predict_features, num_latent_predictions = m.extract_latent_features_to_disk_from_prebatched_windows(\n",
    "        autoencoder_model, \n",
    "        predict_dataset, \n",
    "        latent_features_path, \n",
    "        features_name='latent_features_train',\n",
    "        return_array=True,\n",
    "        verbose=False)\n",
    "    # make preductions \n",
    "    cluster_predictions = cluster_model.predict(latent_predict_features)\n",
    "    # Build the reconstruction of the predicted cluster label data\n",
    "    cluster_img = dp.reconstruct_cluster_image(image_windows_coordinates,window_size, cluster_predictions)\n",
    "    image_name = os.path.basename(dir)\n",
    "    # Path to save latent features to disk\n",
    "    predictions_path = os.path.join(job_data_path, 'predictions')\n",
    "    m.display_reconstructed_and_cluster_images(reconstructed_img,cluster_img,show_overlay=True,\n",
    "                                                save_to_disk=True,\n",
    "                                                predictions_time_stamp=predictions_time_stamp,\n",
    "                                                output_path=job_data_path,\n",
    "                                                image_name=image_name,\n",
    "                                                dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
