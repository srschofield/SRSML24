{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # CNN autoencoder and Clustering from MTRX data\n",
    "\n",
    "Use this notebook to load Scienta Omicron Matrix format SPM data and create standardised images for machine learning training and analysis. The code can generate both JPG image data, useful for manually checking the data, and windowed numpy data that can be loaded into ML models. \n",
    "\n",
    "The notebook then creates an autoencoder for training on a large dataset, followed by KMEANS clustering. \n",
    "\n",
    "**Author**: Steven R. Schofield  \n",
    "**Created**: November, 2024 -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module_path = /hpc/srs/Python/modules\n",
      "data_path = /hpc/srs/Python-data\n"
     ]
    }
   ],
   "source": [
    "# Define path where to find the module. This allows for a different path depending on where the code is running (my mac or the cluster)\n",
    "import os\n",
    "\n",
    "module_path_list = [\n",
    "    '/Users/steven/academic-iCloud/Python/modules',\n",
    "    '/hpc/srs/Python/modules'\n",
    "] \n",
    "\n",
    "data_path_list = [\n",
    "    '/Users/steven/Python-data',\n",
    "    '/hpc/srs/Python-data'\n",
    "]\n",
    "\n",
    "module_path = next((p for p in module_path_list if os.path.exists(p)), None)\n",
    "if not module_path:\n",
    "    exit(\"No valid module paths.\")\n",
    "else:\n",
    "    print('module_path = {}'.format(module_path))\n",
    "\n",
    "data_path = next((p for p in data_path_list if os.path.exists(p)), None)\n",
    "if not module_path:\n",
    "    exit(\"No valid data paths.\")\n",
    "else:\n",
    "    print('data_path = {}'.format(data_path))\n",
    "\n",
    "# adjust tensorflow output level\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # 0 default all messages, 1 warnings and errors, 2, errors, 3 fatal errors only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "\n",
      "Python version: 3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:45:29) \n",
      "[GCC 10.4.0]\n",
      "TensorFlow version: 2.4.1\n",
      "TensorFlow is built with CUDA: True\n",
      "TensorFlow is built with ROCm: False\n",
      "\n",
      "System: Linux 4.18.0-513.24.1.el8_9.x86_64 (x86_64)\n",
      "Platform: Linux-4.18.0-513.24.1.el8_9.x86_64-x86_64-with-glibc2.28\n",
      "Processor: x86_64\n",
      "\n",
      "Number of GPUs available to TensorFlow: 1\n",
      "GPU Device: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "\n",
      ">>> Running with GPU available <<<  (Linux-4.18.0-513.24.1.el8_9.x86_64-x86_64-with-glibc2.28)\n",
      "\n",
      "Current time 2024-12-16 21:56:07\n"
     ]
    }
   ],
   "source": [
    "# # Ensure modules are reloaded \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import standard modules\n",
    "import os, sys\n",
    "import numpy as np\n",
    "\n",
    "import platform\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Add custom module path to list\n",
    "sys.path.append(module_path)\n",
    "\n",
    "# Import custom module\n",
    "import SRSML24.data_prep as dp\n",
    "import SRSML24.model as m\n",
    "\n",
    "import tensorflow as tf\n",
    "#from tensorflow.keras.optimizers.legacy import Adam \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import platform \n",
    "\n",
    "m.print_system_info()\n",
    "\n",
    "start_time = dp.current_datetime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for windows creation\n",
    "job_name = 'MSc2024'\n",
    "job_data_path = dp.create_new_data_path(data_path, job_name, include_date=False)\n",
    "mtrx_train_path = os.path.join(data_path, 'mtrx/train')\n",
    "mtrx_test_path = os.path.join(data_path, 'mtrx/test')\n",
    "mtrx_predict_path = os.path.join(data_path, 'mtrx/predict')\n",
    "flatten_method = 'poly_xy'\n",
    "pixel_density = 15.0    #Â Convert all images to a constant pixel density\n",
    "pixel_ratio = 0.7       # If an image has less than this % in the slow scan direction it is discarded\n",
    "data_scaling = 1.e9     # Scale the z-height of the data\n",
    "window_size = 32        # Window size for training/validation\n",
    "window_pitch = 32       # Window pitch for training/validation\n",
    "together = True        # Set this True to save image windows for a mtrx image as a single file rather than separate files.\n",
    "collate = False         # Set this True to remove all subfolder directories and save all data in root data path\n",
    "\n",
    "# Parameters for training\n",
    "model_name = 'unet_' + job_name\n",
    "batch_size = 128\n",
    "buffer_size = 12800 # shuffling\n",
    "learning_rate = 1e-4\n",
    "epochs = 5\n",
    "\n",
    "# Parameters for clustering\n",
    "cluster_model_name = model_name + '_kmeans'\n",
    "cluster_batch_size = 5120 # This is the number of latent features in a batch for clustering. \n",
    "                          # Does not have to be the same as for training and probably should \n",
    "                          # be larger. \n",
    "cluster_buffer_size = cluster_batch_size * 5    # shuffling buffer\n",
    "num_clusters=20                                # Desired number of clusters (centroids) to form in the data.\n",
    "n_init=50                                       # Number of times the algorithm will run with different centroid seeds.\n",
    "max_iter=1000                                   # Maximum iterations allowed for each mini-batch to refine centroids.\n",
    "reassignment_ratio=0.05                         # Fraction of clusters reassigned per step; lower values stabilize updates.\n",
    "\n",
    "# Parameters for PREDICTIONS\n",
    "predict_window_pitch = 4               # Window pitch for prediction\n",
    "\n",
    "# DATA LIMITS FOR TESTING THE CODE\n",
    "mtrx_train_data_limit = 10000        # Number of MTRX files to process (training)\n",
    "mtrx_test_data_limit = 1000        # Number of MTRX files to process (validation)\n",
    "\n",
    "train_data_limit = None #2000*batch_size\n",
    "predict_data_limit = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # REMOVE ALL DATA FOLDERS EXCEPT MTRX \n",
    "# dp.delete_data_folders(job_data_path, override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3585 files with extension 'Z_mtrx' in directory:\n",
      "/hpc/srs/Python-data/mtrx/train\n",
      "There are 3585 files to process\n",
      "...................t..........t.......................t...t.............ttt.t.ttt.tt.t.....tt.t.ttt 100\n",
      "\bttt.tttttt.........................tt.....ttt..tt.......t.........ttt...............tttttt.....ttttt 200\n",
      "\bttt........................................................ttttttt..t.tttt.tttttttttttttt........... 300\n",
      "...............tt.tttt.....t.tttttttttttttt.......tttttt......t....t..t...................ttttt...t 400\n",
      "\btt...tt...ttt.................................t...........t......tt...........tt.ttt...tt..t..t.tt.t 500\n",
      "..........t.t..ttt.t.tt.t..t..tt.t.t.t.........ttt.t....tt....tttt...t.................tt.......ttt 600\n",
      "\bttt........ttt.........................................ttttttttttt.......tt.....tt.t..tt..ttt.tttttt 700\n",
      "\btttttttt....tt.tttttttttttttttttttt.ttttttt.....t.tttttttt.......ttttttttttttttttttttttttttt.ttttttt 800\n",
      ".tttttttttttttttttttttttttttttttttttt.tttttttttttttttttttttttttttt.tttttt.ttttt.ttttttttttttttttttt 900\n",
      "\bttttttttttttttttttttttttttttttttttttttt.ttt.t.t..ttt.tt.......tt........t...............t........... 1000\n",
      "..........t..t...............t.t...tt....t...........ttttttttttt..t..tttttttttttttttttttttttttttttt 1100\n",
      "\bttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttt.................tttttttttttttt 1200\n",
      "\bt.t....tttttttttttttttttttt.ttttttt...tttttttttttttt.ttttttt.t.tt..t......ttt.t..ttttttt............ 1300\n",
      "........................................................tt.t.t.......t...t......t.....t.....t..tttt 1400\n",
      "\btttt...........tttttt.t...................t..............tttt.t........tttt...t.....t...tt..tt..t... 1500\n",
      "t.....t...t..t.tt...t...t..tt..t..tt....tt..t...t...t....t..t......t......t.t.....t.............t.. 1600\n",
      "...ttttt................t.....................................ttt.........t....t.tt....tt...t...... 1700\n",
      "..t....t........................................................................................... 1800\n",
      "...............................................................ttt..tttt..t...........tt...t.t..... 1900\n",
      "..tttttttttttttttttt.ttttttt.tt................tt..ttt..tt............t.......t......t....tttt.ttt. 2000\n",
      "...............................................................t.t....t.t.......................... 2100\n",
      "...ttttttttttttttttttttttttttttttttttttttttt.......tt..t.ttt............tttttt..tt............t..tt 2200\n",
      "\bt.tttttttt............ttttttt........................................ttt........t.......t.ttttttt... 2300\n",
      "..........tt..........................................................................tt..tt......t 2400\n",
      "\btttt.tttt...ttt.............t........t.......................................t.t...................t 2500\n",
      ".ttttt...............ttttt....tttttttttt...t...tt.t...t................................t........... 2600\n",
      "....t...tttt..............ttttt...ttttttttttttttttttttttttttttttttttt.ttttt.tt.t.t....t.......t.... 2700\n",
      "........t.....ttt...t..t..................ttttttttt......t..............................ttt..tt.... 2800\n",
      "..t.t.t.tt...tttt..tttttt.t.t.t.......t.ttt.tttttttt................ttt.....t.t.t.................. 2900\n",
      ".................................tt.t.t............................tt.........ttttttttt............ 3000\n",
      "..........t.......t....t..ttt.t....tt...t.t.t.t..tt..tt.t.t.t...t...........t..ttttt.tt......t...t. 3100\n",
      "...........tt.t.tt........t.t.t.t.ttt...t.t.t..t.t............t.t.t.t.t.t.t..t..tt.tt.t...t.t.t.... 3200\n",
      "......tttt...ttttttt.t........t..ttt.t...t....t...t..t.t.t.t..t..ttt.t..t.t.......t................ 3300\n",
      ".t.tt.t.........tt..t...........t.tt..t............t.t..t....t.t..tt.t.t.t.....................tt.. 3400\n",
      "..ttttt.......t.t.t...t...tt.t.t.t..t..t.t.......t.t..t.t..t...t.t.ttt..t..t.t......t.t.t..t..t..t. 3500\n",
      "t.t.t.t...t.tt..t......ttt.t.t.t....t.t.t...t.t.t.t..t.t.t.t...t.ttt..t..t...t...t..t\n",
      "********************\n",
      "Conversion complete.\n",
      "********************\n",
      "\n",
      "Found 1393 files with extension 'Z_mtrx' in directory:\n",
      "/hpc/srs/Python-data/mtrx/test\n",
      "There are 1000 files to process\n",
      ".......t..tt..t.t............t........t..............t............t...t..........t..t...tt....t..tt 100\n",
      ".........t..ttt.................t.t.....tt...t.....t.t.....t...t.............................ttt.tt 200\n",
      "\bt..tt..ttttttttttttttt.t.t.tt.t....t...t.t.ttt.t.t.ttt..t..t.t.ttt........tttttttt.ttttttttt.t.ttt.. 300\n",
      "\bt..tttttt.t...tt..t.t......t.....ttt..t.ttt.t.ttttttt..tttttttttt..t.t.tttttt....tt.t............... 400\n",
      "..tttt.t.tttttt.t.........t............tt.t.tttttt.............t......t..t......t..............t... 500\n",
      "..t......t.....ttt.......t.......tt.......t........................................................ 600\n",
      "...............................t...................................t....t...........t...t..t....... 700\n",
      "..tt...ttttt..t.......t..t...tt..........tttt.......................t.t..t.t....t.t...t...t.t.tt... 800\n",
      "tttt.t....t.t.t.t..t......t...t.t.t....tttt.t..t..tt...t.t.t.ttt.tt......tt..tttt..tt.......ttt.... 900\n",
      "t..t..ttt.......t.............t....tt......t.........t....ttt.t...............tt.........t..ttt..tt 1000\n",
      "\bt\n",
      "********************\n",
      "Conversion complete.\n",
      "********************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training data\n",
    "mtrx_train_file_list, _ = dp.list_files_by_extension(mtrx_train_path,'Z_mtrx',verbose=False)\n",
    "\n",
    "dp.process_mtrx_files(\n",
    "    mtrx_train_file_list[0:mtrx_train_data_limit],\n",
    "    job_data_path, # save data path\n",
    "    flatten_method = flatten_method, pixel_density = pixel_density, pixel_ratio = pixel_ratio,\n",
    "    data_scaling = data_scaling, window_size = window_size, window_pitch = window_pitch,\n",
    "    save_windows = True,\n",
    "    save_jpg = True,\n",
    "    together = together,\n",
    "    collate = collate,\n",
    "    verbose = False\n",
    "    )\n",
    "\n",
    "# Test data\n",
    "mtrx_test_file_list, _ = dp.list_files_by_extension(mtrx_test_path,'Z_mtrx',verbose=False)\n",
    "\n",
    "dp.process_mtrx_files(\n",
    "    mtrx_test_file_list[0:mtrx_test_data_limit],\n",
    "    job_data_path, # save data path\n",
    "    flatten_method = flatten_method, pixel_density = pixel_density, pixel_ratio = pixel_ratio,\n",
    "    data_scaling = data_scaling, window_size = window_size, window_pitch = window_pitch,\n",
    "    save_windows = True,\n",
    "    save_jpg = True,\n",
    "    together = together,\n",
    "    collate = collate,\n",
    "    verbose = False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # Set up tensorflow data pipeline -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training data - tensorflow data pipeline for autoencoder\n",
    "# windows_train_path = os.path.join(job_data_path, 'windows/train')\n",
    "# train_files, num_train = dp.list_files_by_extension(windows_train_path, 'npy')\n",
    "# train_files = train_files[:train_data_limit]\n",
    "# # Create dataset with prefetching\n",
    "# #train_dataset = m.create_tf_dataset(train_files, batch_size=batch_size, buffer_size=buffer_size, is_autoencoder=True, shuffle=True)\n",
    "# train_dataset = m.create_tf_dataset_batched(\n",
    "#     train_files, \n",
    "#     batch_size=batch_size, \n",
    "#     buffer_size=buffer_size, \n",
    "#     window_size=window_size,\n",
    "#     is_autoencoder=True, \n",
    "#     shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Validation data - tensorflow data pipeline for autoencoder\n",
    "# windows_test_path = os.path.join(job_data_path, 'windows/test')\n",
    "# test_files, num_test = dp.list_files_by_extension(windows_test_path, 'npy')\n",
    "# test_files = test_files[:train_data_limit]\n",
    "\n",
    "# # Create dataset with prefetching\n",
    "# test_dataset = m.create_tf_dataset_batched(\n",
    "#     test_files, \n",
    "#     batch_size=batch_size, \n",
    "#     buffer_size=buffer_size, \n",
    "#     window_size=window_size,\n",
    "#     is_autoencoder=True, \n",
    "#     shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # Autoencoder build and train -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build and compile the UNET model\n",
    "# autoencoder_model = m.build_autoencoder(window_size=window_size,model_name=model_name)\n",
    "# autoencoder_model.summary()\n",
    "\n",
    "# # Check if running on Apple Silicon\n",
    "# is_mac_silicon = platform.system() == \"Darwin\" and platform.processor() == \"arm\"\n",
    "\n",
    "# if is_mac_silicon:\n",
    "#     print(\"Detected Mac with Apple Silicon. Compiling the model with the legacy RMSprop optimizer for compatibility with TensorFlow-metal.\")\n",
    "#     autoencoder_model.compile(\n",
    "#         optimizer=tf.keras.optimizers.legacy.RMSprop(learning_rate=learning_rate),\n",
    "#         loss='mean_squared_error',\n",
    "#         metrics=['mse', 'mae']\n",
    "#     )\n",
    "# else:\n",
    "#     print(\"Compiling the model with the RMSprop optimizer.\")\n",
    "#     autoencoder_model.compile(\n",
    "#         optimizer=tf.keras.optimizers.RMSprop(learning_rate=learning_rate),\n",
    "#         loss='mean_squared_error',\n",
    "#         metrics=['mse', 'mae']\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train the model using the tf.data datasets\n",
    "# history = autoencoder_model.fit(\n",
    "#     train_dataset,\n",
    "#     validation_data=test_dataset,\n",
    "#     epochs=epochs,\n",
    "#     shuffle=True,\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# # Save the model as soon as training completes\n",
    "# model_path = os.path.join(job_data_path,'model')\n",
    "# m.save_model(autoencoder_model, model_path, model_name=model_name)\n",
    "\n",
    "# end_time = dp.current_datetime()\n",
    "# dp.elapsed_time(start_time,end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot training history\n",
    "# m.plot_training_history(history, \n",
    "#                         loss_name='loss', \n",
    "#                         val_loss_name='val_loss', \n",
    "#                         metric_names=['mse', 'mae'], \n",
    "#                         save_to_disk=True,\n",
    "#                         val_metric_names=['val_mse', 'val_mae'],\n",
    "#                         output_path=model_path,\n",
    "#                         dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # Extract latent features using autoencoder model\n",
    "Will save the latent features also to disk so this step needs to run only once for a given set of parameters -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No target folders found to delete.\n"
     ]
    }
   ],
   "source": [
    "dp.delete_data_folders(\n",
    "    job_data_path, \n",
    "    subdirectories='latent_features',\n",
    "    override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4088 files with extension 'npy' in directory:\n",
      "/hpc/srs/Python-data/MSc2024/windows/train\n",
      "Data pipeline created with 4088 files, batch size: 5120, window size: 32\n",
      "Sample batch shape: (5120, 32, 32, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training data - tensorflow data pipeline \n",
    "windows_train_path = os.path.join(job_data_path, 'windows/train')\n",
    "train_files, num_train = dp.list_files_by_extension(windows_train_path, 'npy')\n",
    "train_files = train_files[:train_data_limit]\n",
    "\n",
    "train_dataset = m.create_tf_dataset_batched(\n",
    "    train_files, \n",
    "    batch_size=cluster_batch_size, \n",
    "    buffer_size=cluster_buffer_size, \n",
    "    window_size=window_size,\n",
    "    is_autoencoder=True, \n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "SavedModel file does not exist at: /hpc/srs/Python-data/MSc2024/model/unet_all_data_2023.keras/{saved_model.pbtxt|saved_model.pb}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m model_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(job_data_path,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munet_all_data_2023\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m autoencoder_model \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Python/modules/SRSML24/model.py:283\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(model_path, model_name, compile_model)\u001b[0m\n\u001b[1;32m    280\u001b[0m model_file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_path, model_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    282\u001b[0m \u001b[38;5;66;03m# Load the model without compiling it\u001b[39;00m\n\u001b[0;32m--> 283\u001b[0m loaded_model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompile_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel loaded from: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loaded_model\n",
      "File \u001b[0;32m~/local/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/keras/saving/save.py:211\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    209\u001b[0m     filepath \u001b[38;5;241m=\u001b[39m path_to_string(filepath)\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath, six\u001b[38;5;241m.\u001b[39mstring_types):\n\u001b[0;32m--> 211\u001b[0m       \u001b[43mloader_impl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_saved_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m saved_model_load\u001b[38;5;241m.\u001b[39mload(filepath, \u001b[38;5;28mcompile\u001b[39m, options)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnable to load model. Filepath is not an hdf5 file (or h5py is not \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavailable) or SavedModel.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/local/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/saved_model/loader_impl.py:111\u001b[0m, in \u001b[0;36mparse_saved_model\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot parse file \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (path_to_pbtxt, \u001b[38;5;28mstr\u001b[39m(e)))\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSavedModel file does not exist at: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m|\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m    112\u001b[0m                 (export_dir,\n\u001b[1;32m    113\u001b[0m                  constants\u001b[38;5;241m.\u001b[39mSAVED_MODEL_FILENAME_PBTXT,\n\u001b[1;32m    114\u001b[0m                  constants\u001b[38;5;241m.\u001b[39mSAVED_MODEL_FILENAME_PB))\n",
      "\u001b[0;31mOSError\u001b[0m: SavedModel file does not exist at: /hpc/srs/Python-data/MSc2024/model/unet_all_data_2023.keras/{saved_model.pbtxt|saved_model.pb}"
     ]
    }
   ],
   "source": [
    "# Load the trained autoencoder model\n",
    "model_path = os.path.join(job_data_path,'model')\n",
    "model_name = 'unet_all_data_2023'\n",
    "autoencoder_model = m.load_model(model_path, model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to save latent features to disk\n",
    "latent_features_path = os.path.join(job_data_path, 'latent_features')\n",
    "\n",
    "m.extract_latent_features_to_disk_from_prebatched_windows(\n",
    "    autoencoder_model, \n",
    "    train_dataset, \n",
    "    latent_features_path, \n",
    "    features_name='latent_features_train',\n",
    "    return_array=False,\n",
    "    verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # Create tensorflow dataset pipeline for latent features -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List and sort latent feature files\n",
    "latent_features_path = os.path.join(job_data_path, 'latent_features')\n",
    "latent_features_files, num_latent_files = dp.list_files_by_extension(latent_features_path, 'npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the latent features from disk into a tensor dataset pipeline\n",
    "latent_features_dataset = m.create_latent_features_tf_dataset(\n",
    "    latent_features_files,\n",
    "    batch_size=cluster_batch_size,\n",
    "    shuffle=True, \n",
    "    shuffle_buffer_size=cluster_buffer_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # Train KMEANS cluster model -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_model, convergence_history = m.train_kmeans(\n",
    "    latent_features_dataset,                # tf.data.Dataset containing batches of latent feature vectors.\n",
    "    batch_size=cluster_batch_size,          # Size of each batch for the KMeans model (controls memory usage and stability).\n",
    "    num_clusters=num_clusters,              # Desired number of clusters (centroids) to form in the data.\n",
    "    n_init=n_init,                          # Number of times the algorithm will run with different centroid seeds.\n",
    "    max_iter=max_iter,                      # Maximum iterations allowed for each mini-batch to refine centroids.\n",
    "    reassignment_ratio=reassignment_ratio   # Fraction of clusters reassigned per step; lower values stabilize updates.\n",
    ")\n",
    "\n",
    "# Save cluster model\n",
    "cluster_model_path = os.path.join(job_data_path,'cluster_model')\n",
    "m.save_cluster_model(cluster_model, cluster_model_path, model_name=cluster_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # Process Predictions MTRX data -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp.delete_data_folders(\n",
    "    job_data_path, \n",
    "    subdirectories='jpg',\n",
    "    override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction data\n",
    "mtrx_predict_file_list, _ = dp.list_files_by_extension(mtrx_predict_path,'Z_mtrx',verbose=False)\n",
    "\n",
    "data_scaling = 1.e9     # Scale the z-height of the data\n",
    "\n",
    "flatten_method = 'row_and_poly_xy_deg_2'\n",
    "flatten_method = 'row_mean'\n",
    "flatten_method = 'iterate_mask'\n",
    "flatten_method = 'poly_xy'\n",
    "\n",
    "dp.process_mtrx_files(\n",
    "    mtrx_predict_file_list,\n",
    "    job_data_path, # save data path\n",
    "    flatten_method = flatten_method, pixel_density = pixel_density, pixel_ratio = pixel_ratio,\n",
    "    data_scaling = data_scaling, window_size = window_size, window_pitch = predict_window_pitch,\n",
    "    save_windows = True,\n",
    "    save_jpg = True,\n",
    "    collate = False,\n",
    "    verbose = False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # Make data predictions -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained autoencoder\n",
    "model_path = os.path.join(job_data_path,'model')\n",
    "autoencoder_model = m.load_model(model_path, model_name=model_name)\n",
    "\n",
    "#Â Load a previously saved cluster model from disk\n",
    "cluster_model_path = os.path.join(job_data_path,'cluster_model')\n",
    "cluster_model = m.load_cluster_model(cluster_model_path, model_name=cluster_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of folders containing predict window data\n",
    "windows_predict_path = os.path.join(job_data_path, 'windows/predict')\n",
    "predict_data_files_list = dp. find_bottom_layer_folders(windows_predict_path)\n",
    "predict_data_files_list = predict_data_files_list[:predict_data_limit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_time_stamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "for dir in predict_data_files_list:\n",
    "    #Limit viewing to FU images\n",
    "    if not dir.endswith(\"FU\"):\n",
    "        print('Not FU so skipping: {}'.format(dir))\n",
    "        continue  \n",
    "\n",
    "    #Â get list of the individual file names of the windows\n",
    "    window_list, num_windows = dp.list_files_by_extension(dir, extension=\".npy\", verbose=False)\n",
    "    # load the numpy windows\n",
    "    image_windows = dp.load_n_numpy_files(window_list,num_windows)\n",
    "    # load the coordinate files for the windows for window reconstruction\n",
    "    image_windows_coordinates = dp.load_coordinates_file(dir)\n",
    "    # Build the reconstruction of the original image\n",
    "    reconstructed_img = dp.reconstruct_image(image_windows,image_windows_coordinates,window_size)\n",
    "\n",
    "    #Â create a tf dataset from the numpy windows for making the latent features\n",
    "    predict_dataset = m.create_tf_dataset(window_list, batch_size=num_windows, buffer_size=num_windows, shuffle=False, is_autoencoder=False)\n",
    "    # make the latent features for each window using the autoencoder model \n",
    "    latent_predict_features, num_latent_predictions = m.extract_latent_features_to_disk_from_prebatched_windows(\n",
    "        autoencoder_model, \n",
    "        predict_dataset, \n",
    "        latent_features_path, \n",
    "        features_name='latent_features_train',\n",
    "        return_array=True,\n",
    "        verbose=False)\n",
    "    # make preductions \n",
    "    cluster_predictions = cluster_model.predict(latent_predict_features)\n",
    "    #Â Build the reconstruction of the predicted cluster label data\n",
    "    cluster_img = dp.reconstruct_cluster_image(image_windows_coordinates,window_size, cluster_predictions)\n",
    "    image_name = os.path.basename(dir)\n",
    "    # Path to save latent features to disk\n",
    "    predictions_path = os.path.join(job_data_path, 'predictions')\n",
    "    m.display_reconstructed_and_cluster_images(reconstructed_img,cluster_img,show_overlay=True,\n",
    "                                                save_to_disk=True,\n",
    "                                                predictions_time_stamp=predictions_time_stamp,\n",
    "                                                output_path=job_data_path,\n",
    "                                                image_name=image_name,\n",
    "                                                dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
